Perfect ðŸ‘Œ Now we are moving into **Day 31â€“32: Affinity & Anti-Affinity**, which is a **very deep and powerful scheduling concept in Kubernetes**.
This is where youâ€™ll learn how to **control pod placement** based on rules â€” ensuring better **availability, performance, and fault-tolerance**.

Iâ€™ll cover **concepts â†’ YAML examples â†’ hands-on labs â†’ monitoring â†’ best practices â†’ troubleshooting** in depth.

---

# **Day 31â€“32: Affinity & Anti-Affinity**

---

## **1. Why Affinity/Anti-Affinity Matters**

By default, Kubernetes scheduler only considers:

* **Resource requests (CPU/memory)**
* **Taints & tolerations**
* **Node selectors/labels**

But in **real-world production**, you often need more fine-grained placement rules:

* Run **related pods together** (data locality, caching).
* Run **critical replicas apart** (HA, fault tolerance).
* Run workloads on **specific zones/racks** for cost/latency optimization.

ðŸ‘‰ Affinity/Anti-Affinity rules let you **influence the scheduler beyond basic constraints**.

---

## **2. Key Concepts**

### **Types of Affinity**

1. **Node Affinity**

   * Controls which **nodes** a pod can schedule on.
   * Uses **node labels**.
   * Similar to `nodeSelector` but more expressive.

   Two modes:

   * `requiredDuringSchedulingIgnoredDuringExecution` â†’ **hard rule** (must be satisfied).
   * `preferredDuringSchedulingIgnoredDuringExecution` â†’ **soft rule** (scheduler tries, but not mandatory).

---

2. **Pod Affinity**

   * Controls scheduling based on **other podsâ€™ labels**.
   * Example: Web pod should run **with** cache pod on same node (data locality).

---

3. **Pod Anti-Affinity**

   * Opposite of pod affinity.
   * Example: Ensure **replicas of same app donâ€™t land on same node** â†’ high availability.

---

## **3. Node Affinity â€“ Example**

### Label nodes

```bash
kubectl label nodes worker1 disktype=ssd
kubectl label nodes worker2 disktype=hdd
```

### Pod with Node Affinity (hard requirement)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-demo
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: app
    image: nginx
```

ðŸ‘‰ This pod will **only run on nodes with `disktype=ssd`**.

---

## **4. Pod Affinity â€“ Example**

### Label existing pod

```bash
kubectl run cache --image=redis -l app=cache
```

### Pod that requires affinity with `cache` pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-affinity-demo
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cache
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: app
    image: nginx
```

ðŸ‘‰ This pod will **only schedule on nodes where a pod with label `app=cache` is running**.

---

## **5. Pod Anti-Affinity â€“ Example**

### Deployment with anti-affinity (spread replicas across nodes)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: nginx
        image: nginx
```

ðŸ‘‰ Each replica will be scheduled on a **different node (hostname)** â†’ avoiding single-node failures.

---

## **6. topologyKey**

* Defines **scheduling domain**.
* Common values:

  * `kubernetes.io/hostname` â†’ per node.
  * `topology.kubernetes.io/zone` â†’ per zone.
  * `topology.kubernetes.io/region` â†’ per region.

ðŸ‘‰ Helps spread workloads across nodes, zones, or regions.

---

## **7. Hands-On Lab**

### Step 1: Label nodes

```bash
kubectl label node worker1 zone=zoneA
kubectl label node worker2 zone=zoneB
```

### Step 2: Deploy workload with pod anti-affinity

* Ensure replicas spread across **zoneA & zoneB**.

### Step 3: Observe scheduling

```bash
kubectl get pods -o wide
```

ðŸ‘‰ Check on which node each pod is running.

### Step 4: Remove a label

```bash
kubectl label node worker2 zone-
```

ðŸ‘‰ See how scheduling changes (pending pods).

---

## **8. Best Practices**

âœ… Always use **podAntiAffinity** for high availability (critical apps).
âœ… Use **soft rules (preferred)** to allow flexibility during node failures.
âœ… Label nodes meaningfully (`zone`, `rack`, `disktype`, `gpu`).
âœ… Combine **affinity with taints/tolerations** for stronger placement control.
âœ… Avoid too strict rules â†’ may cause pods to stay in **Pending**.

---

## **9. Troubleshooting**

* **Pods Pending** â†’ no node matches affinity/anti-affinity rules.
* **Cluster underutilized** â†’ too strict placement prevents scheduling.
* **Imbalanced workloads** â†’ check labels/topology keys.
* **Debug scheduling decisions**:

```bash
kubectl describe pod <pod-name>
```

Look at **Events** â†’ scheduler will log why it failed.

---

## âœ… Key Takeaways

* **Node affinity** â†’ match pods to specific nodes (labels).
* **Pod affinity** â†’ co-locate pods (same node/zone).
* **Pod anti-affinity** â†’ spread pods across nodes/zones.
* Use **topologyKey** wisely (`hostname`, `zone`, `region`).
* Always balance **availability vs scheduling flexibility**.


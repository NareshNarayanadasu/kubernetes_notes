Perfect ğŸ‘ Now weâ€™re at **Day 35: Pod Disruption Budget (PDB) Lab** â€” this is where we make sure that **applications stay highly available** even during **voluntary disruptions** like node upgrades, draining, or rescheduling.

Iâ€™ll cover **concepts â†’ YAML â†’ hands-on lab â†’ monitoring â†’ best practices â†’ troubleshooting** in depth.

---

# **Day 35: Pod Disruption Budget (PDB) Lab**

---

## **1. Why Pod Disruption Budget Matters**

* Kubernetes may **evict pods** during:

  * Node drain (e.g., upgrade/maintenance).
  * Cluster autoscaling (removing nodes).
  * Admin `kubectl drain` or voluntary disruptions.

Without protection:

* All replicas of an application could be killed at once â†’ **downtime**.

ğŸ‘‰ **PDB ensures a minimum number of pods remain available.**

---

## **2. Key Concepts**

* **PDB (PodDisruptionBudget)** defines how many pods of an application must always remain running.
* It applies only to **voluntary disruptions** (node drain, autoscaler scale-down).
  âš ï¸ It does **not** protect against involuntary disruptions (node crash, OOM).

Two main ways to configure:

1. **minAvailable** â†’ Minimum pods that must always be running.
2. **maxUnavailable** â†’ Maximum pods that can be disrupted at once.

---

## **3. Example: PDB with minAvailable**

### Deployment (3 replicas)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx
```

### PodDisruptionBudget

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web
```

ğŸ‘‰ At least **2 pods** must remain available at all times.
ğŸ‘‰ During a node drain, only **1 pod can be disrupted at a time**.

---

## **4. Example: PDB with maxUnavailable**

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: api
```

ğŸ‘‰ At most **1 pod** can be down during disruption.
ğŸ‘‰ Works well for stateful apps where you know disruption tolerance.

---

## **5. Hands-On Lab**

### Step 1: Deploy app + PDB

```bash
kubectl apply -f web-deployment.yaml
kubectl apply -f web-pdb.yaml
```

### Step 2: Verify PDB

```bash
kubectl get pdb
kubectl describe pdb web-pdb
```

ğŸ‘‰ Youâ€™ll see `Allowed disruptions: 1`.

### Step 3: Simulate Node Drain

```bash
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```

ğŸ‘‰ Kubernetes will:

* Evict pods **gradually**, respecting the PDB.
* At least 2 replicas will remain running.

### Step 4: Check pods

```bash
kubectl get pods -o wide
```

---

## **6. Monitoring**

* Use `kubectl describe pdb` to see disruption status.
* Track **pod eviction events** in `kubectl get events`.
* In Prometheus, monitor **pod eviction metrics** (`kube_pod_disruption_budget_*`).

---

## **7. Best Practices**

âœ… Always use **PDBs for HA apps** (Deployments, StatefulSets).
âœ… For DBs (like MongoDB, Cassandra), set `minAvailable` to **quorum size**.
âœ… For stateless apps, use `maxUnavailable` for **rolling upgrades**.
âœ… Donâ€™t set `minAvailable=replicas` â†’ prevents any upgrades.
âœ… Always test PDB with **node drain simulation**.

---

## **8. Troubleshooting**

* **Drain command hangs** â†’ PDB too strict (e.g., `minAvailable=all pods`).
* **Pods unavailable despite PDB** â†’ Involuntary disruption (node crash).
* **Cluster autoscaler fails to remove nodes** â†’ PDB blocks eviction.
* **Check why eviction failed**:

```bash
kubectl get events --field-selector reason=FailedDisruptionBudget
```

---

## âœ… Key Takeaways

* **PDB protects applications during voluntary disruptions.**
* Use `minAvailable` (keep N running) or `maxUnavailable` (allow N disruptions).
* Combine PDB with **replicasets, HPA, affinity/anti-affinity** for real HA.
* Always simulate node drain to validate your disruption strategy.

---


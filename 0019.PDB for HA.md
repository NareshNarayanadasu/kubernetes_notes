Perfect 👍 Now we’re at **Day 35: Pod Disruption Budget (PDB) Lab** — this is where we make sure that **applications stay highly available** even during **voluntary disruptions** like node upgrades, draining, or rescheduling.

I’ll cover **concepts → YAML → hands-on lab → monitoring → best practices → troubleshooting** in depth.

---

# **Day 35: Pod Disruption Budget (PDB) Lab**

---

## **1. Why Pod Disruption Budget Matters**

* Kubernetes may **evict pods** during:

  * Node drain (e.g., upgrade/maintenance).
  * Cluster autoscaling (removing nodes).
  * Admin `kubectl drain` or voluntary disruptions.

Without protection:

* All replicas of an application could be killed at once → **downtime**.

👉 **PDB ensures a minimum number of pods remain available.**

---

## **2. Key Concepts**

* **PDB (PodDisruptionBudget)** defines how many pods of an application must always remain running.
* It applies only to **voluntary disruptions** (node drain, autoscaler scale-down).
  ⚠️ It does **not** protect against involuntary disruptions (node crash, OOM).

Two main ways to configure:

1. **minAvailable** → Minimum pods that must always be running.
2. **maxUnavailable** → Maximum pods that can be disrupted at once.

---

## **3. Example: PDB with minAvailable**

### Deployment (3 replicas)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx
```

### PodDisruptionBudget

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web
```

👉 At least **2 pods** must remain available at all times.
👉 During a node drain, only **1 pod can be disrupted at a time**.

---

## **4. Example: PDB with maxUnavailable**

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: api
```

👉 At most **1 pod** can be down during disruption.
👉 Works well for stateful apps where you know disruption tolerance.

---

## **5. Hands-On Lab**

### Step 1: Deploy app + PDB

```bash
kubectl apply -f web-deployment.yaml
kubectl apply -f web-pdb.yaml
```

### Step 2: Verify PDB

```bash
kubectl get pdb
kubectl describe pdb web-pdb
```

👉 You’ll see `Allowed disruptions: 1`.

### Step 3: Simulate Node Drain

```bash
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```

👉 Kubernetes will:

* Evict pods **gradually**, respecting the PDB.
* At least 2 replicas will remain running.

### Step 4: Check pods

```bash
kubectl get pods -o wide
```

---

## **6. Monitoring**

* Use `kubectl describe pdb` to see disruption status.
* Track **pod eviction events** in `kubectl get events`.
* In Prometheus, monitor **pod eviction metrics** (`kube_pod_disruption_budget_*`).

---

## **7. Best Practices**

✅ Always use **PDBs for HA apps** (Deployments, StatefulSets).
✅ For DBs (like MongoDB, Cassandra), set `minAvailable` to **quorum size**.
✅ For stateless apps, use `maxUnavailable` for **rolling upgrades**.
✅ Don’t set `minAvailable=replicas` → prevents any upgrades.
✅ Always test PDB with **node drain simulation**.

---

## **8. Troubleshooting**

* **Drain command hangs** → PDB too strict (e.g., `minAvailable=all pods`).
* **Pods unavailable despite PDB** → Involuntary disruption (node crash).
* **Cluster autoscaler fails to remove nodes** → PDB blocks eviction.
* **Check why eviction failed**:

```bash
kubectl get events --field-selector reason=FailedDisruptionBudget
```

---

## ✅ Key Takeaways

* **PDB protects applications during voluntary disruptions.**
* Use `minAvailable` (keep N running) or `maxUnavailable` (allow N disruptions).
* Combine PDB with **replicasets, HPA, affinity/anti-affinity** for real HA.
* Always simulate node drain to validate your disruption strategy.

---


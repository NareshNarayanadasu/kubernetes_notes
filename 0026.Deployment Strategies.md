Absolutely! Let’s deep-dive into **Day 47–48: Deployment Strategies**, covering **concepts → Kubernetes implementations → hands-on examples → canary lab → best practices → troubleshooting**. I’ll cover everything in detail so you don’t miss anything.

---

# **Day 47–48: Deployment Strategies**

---

## **1. Why Deployment Strategies Matter**

* Avoid downtime during application updates.
* Minimize risk of introducing bugs in production.
* Ensure smooth user experience during upgrades.

Common strategies:

1. **Rolling Updates** – update pods gradually.
2. **Blue-Green Deployment** – run two identical environments, switch traffic.
3. **Canary Deployment** – release to small subset of users, gradually increase traffic.

---

## **2. Rolling Updates (Default in Kubernetes)**

* Update pods gradually to new version.
* Old pods terminated only after new pods ready.
* Parameters controlled via Deployment spec:

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 25%
    maxSurge: 25%
```

**Explanation:**

* `maxUnavailable`: % of old pods that can be unavailable during update.
* `maxSurge`: % of extra pods allowed above desired replicas.

**Hands-on:**

```bash
kubectl create deployment nginx --image=nginx:1.21
kubectl scale deployment nginx --replicas=4
# Update image
kubectl set image deployment/nginx nginx=nginx:1.22
kubectl rollout status deployment/nginx
kubectl rollout history deployment/nginx
kubectl rollout undo deployment/nginx
```

---

## **3. Blue-Green Deployment**

* Two environments: **Blue** (current) & **Green** (new version).
* Traffic switched after validation.
* Pros: zero downtime, quick rollback.
* Cons: requires double infrastructure (resource-heavy).

**Kubernetes Implementation:**

1. Deploy new version with different deployment name.
2. Update Service selector to point to new deployment.

```yaml
# blue service points to blue deployment
selector:
  app: myapp
  version: blue
```

3. Switch service selector to green deployment after validation.

```bash
kubectl apply -f green-deployment.yaml
kubectl patch svc myapp -p '{"spec":{"selector":{"version":"green"}}}'
```

4. Delete old (blue) deployment once validated.

---

## **4. Canary Deployment**

* Release new version to **small subset** of users.
* Gradually increase traffic to new version.
* Pros: low-risk, metrics-based validation.
* Cons: requires traffic splitting (service mesh or ingress controller).

**Approaches:**

1. **Manual pod split**:

```bash
# old version
kubectl apply -f backend-v1.yaml
# new canary
kubectl apply -f backend-v2-canary.yaml
```

* Service can use **label selector** for weight split (with ingress or service mesh).

2. **Using Ingress / Service Mesh (Istio / Linkerd)**:

* Route 10% traffic to new version:

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: backend
spec:
  hosts:
  - backend.myapp.svc.cluster.local
  http:
  - route:
    - destination:
        host: backend
        subset: v1
      weight: 90
    - destination:
        host: backend
        subset: v2
      weight: 10
```

* Gradually increase v2 weight to 100% after monitoring metrics.

---

## **5. Hands-On Canary Deployment Lab**

### Step 1: Deploy old version

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-v1
spec:
  replicas: 5
  selector:
    matchLabels:
      app: backend
      version: v1
  template:
    metadata:
      labels:
        app: backend
        version: v1
    spec:
      containers:
      - name: backend
        image: myorg/backend:1.0
        ports:
        - containerPort: 8080
```

### Step 2: Deploy canary version

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
      version: v2
  template:
    metadata:
      labels:
        app: backend
        version: v2
    spec:
      containers:
      - name: backend
        image: myorg/backend:2.0
        ports:
        - containerPort: 8080
```

### Step 3: Configure Service for weighted traffic

* If using plain Kubernetes Service:

  * Cannot split traffic by weight; need **Ingress / Service Mesh**.
* If using Istio (recommended for canaries):

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: backend
spec:
  host: backend
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: backend
spec:
  hosts:
  - backend
  http:
  - route:
    - destination:
        host: backend
        subset: v1
      weight: 80
    - destination:
        host: backend
        subset: v2
      weight: 20
```

### Step 4: Monitor metrics

* Use Prometheus/Grafana to check:

  * Error rate
  * Latency
  * Resource usage
* Gradually increase v2 weight to 100% if stable.

### Step 5: Cleanup old deployment

```bash
kubectl delete deployment backend-v1
```

---

## **6. Best Practices**

* Always **monitor metrics** before increasing canary traffic.
* Use **automated rollback triggers** if errors spike.
* Maintain **version labels** for safe traffic routing.
* Use **readiness/liveness probes** for smooth rolling updates.
* Limit **resource spikes** for new version in canary.
* Blue-green recommended for **critical apps** where downtime is unacceptable.
* Canary recommended for **high-traffic apps** with service mesh / ingress capable of weighted routing.

---

## **7. Troubleshooting**

* Canary pods not receiving traffic → check ingress/service mesh routing rules.
* Old pods not terminating in rolling update → check readiness/liveness probes.
* Rollout stuck → `kubectl rollout status deployment/<deployment>` for details.
* Service selector misconfigured → double-check labels and selectors.
* Metrics not showing errors → ensure Prometheus scrape targets include canary pods.

---



Got it! Let’s create a **super in-depth, step-by-step guide for Day 54–55: Upgrades & Patching**, breaking down **every concept, every command, and every consideration** so that nothing is skipped and you can truly understand Kubernetes upgrades. I’ll go slow, explain each step, and include examples, diagrams, and best practices.

---

# **Day 54–55: Kubernetes Upgrades & Patching – Complete In-Depth Guide**

---

## **1. Why Upgrades and Patching Are Important**

Kubernetes is evolving **fast**, with new releases every \~3 months. Not upgrading can cause:

1. **Security risks**: Old versions may have vulnerabilities.
2. **Bug issues**: Older Kubernetes versions may have unresolved bugs.
3. **Compatibility problems**: Workloads may use deprecated APIs.
4. **Performance limitations**: New versions bring optimizations.

**Types of upgrades/patches:**

| Type                      | Description                                                       |
| ------------------------- | ----------------------------------------------------------------- |
| **Minor version upgrade** | e.g., 1.25 → 1.26. Adds new features, removes deprecated APIs.    |
| **Patch upgrade**         | e.g., 1.26.1 → 1.26.2. Mostly security fixes and minor bug fixes. |
| **Node upgrade**          | Updating worker nodes to match control plane.                     |
| **Control plane upgrade** | Updating API server, scheduler, controller-manager, etcd.         |

**Key principle:** Always upgrade **control plane first**, then nodes.

---

## **2. Upgrade Strategies**

### **2.1 Rolling Upgrade**

* Upgrade one node at a time.
* Ensures high availability.
* Steps:

  1. Drain node (evict pods)
  2. Upgrade node
  3. Uncordon node (resume scheduling)
* Repeat for each node.

### **2.2 Blue-Green / Canary Upgrade**

* Blue: Old version
* Green: New version
* Shift traffic gradually.
* Useful for production apps with zero downtime.

### **2.3 Managed vs Self-Hosted**

| Aspect                | Managed (EKS/GKE/AKS)             | Self-Hosted (kubeadm)                         |
| --------------------- | --------------------------------- | --------------------------------------------- |
| Control plane upgrade | Provider handles automatically    | Admin upgrades manually using kubeadm         |
| Node upgrade          | Managed or manual via node groups | Manual rolling upgrade with kubeadm           |
| Risk                  | Lower                             | Higher, need careful planning                 |
| Learning              | Limited                           | Very high, understand all internal components |

---

## **3. Pre-Upgrade Preparation**

### **3.1 Backup Cluster**

* Always back up **etcd** before upgrade.

```bash
# Create etcd snapshot
ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=<etcd-endpoints> \
  --cacert=<ca-file> \
  --cert=<cert-file> \
  --key=<key-file>
```

* For production: Use **Velero** to backup namespaces, PVs, and resources.

### **3.2 Verify Current Versions**

```bash
kubectl version --short
kubectl get nodes
kubectl get pods -A
```

* Note control plane and kubelet versions.

### **3.3 Check API Deprecations**

* Check release notes for deprecated APIs.
* Example: `extensions/v1beta1/Deployment` deprecated → use `apps/v1/Deployment`.

### **3.4 Prepare Maintenance Window**

* Inform team about downtime.
* Schedule upgrades during **off-peak hours**.

---

## **4. Upgrading Managed Clusters**

### **4.1 Amazon EKS**

1. Check available versions:

```bash
aws eks describe-cluster --name dev-cluster --query "cluster.version"
```

2. Upgrade control plane:

```bash
aws eks update-cluster-version \
  --name dev-cluster \
  --kubernetes-version 1.30
```

3. Upgrade worker nodes:

* Create new node group with updated version.
* Drain old nodes:

```bash
kubectl drain <old-node-name> --ignore-daemonsets --delete-emptydir-data
kubectl delete node <old-node-name>
```

4. Validate:

```bash
kubectl get nodes
kubectl get pods -A
```

### **4.2 Google GKE**

```bash
# Upgrade control plane
gcloud container clusters upgrade dev-cluster --master --zone us-central1-a

# Upgrade node pool
gcloud container clusters upgrade dev-cluster --node-pool default-pool --zone us-central1-a
```

### **4.3 Azure AKS**

```bash
az aks upgrade --resource-group myResourceGroup --name dev-cluster --kubernetes-version 1.30.0
```

**Notes for Managed Clusters:**

* Upgrades are **tested by cloud providers**.
* Usually zero downtime for control plane.
* Always **upgrade add-ons** (CoreDNS, metrics-server) after control plane.

---

## **5. Upgrading Self-Hosted Cluster (kubeadm)**

### **5.1 Step 1: Upgrade kubeadm**

```bash
sudo apt-get update
sudo apt-get install -y kubeadm=1.30.0-00
sudo apt-mark hold kubeadm
```

### **5.2 Step 2: Plan Upgrade**

```bash
sudo kubeadm upgrade plan
```

* Shows available versions.
* Shows which nodes can be upgraded.

### **5.3 Step 3: Upgrade Control Plane**

```bash
sudo kubeadm upgrade apply v1.30.0
```

* Upgrades API server, controller-manager, scheduler.
* Control plane pods are restarted one by one.

### **5.4 Step 4: Upgrade kubelet and kubectl**

```bash
sudo apt-get install -y kubelet=1.30.0-00 kubectl=1.30.0-00
sudo systemctl restart kubelet
```

### **5.5 Step 5: Upgrade Worker Nodes**

1. Drain node:

```bash
kubectl drain <worker-node> --ignore-daemonsets --delete-emptydir-data
```

2. Upgrade kubeadm:

```bash
sudo apt-get install -y kubeadm=1.30.0-00
```

3. Upgrade kubelet/kubectl and restart:

```bash
sudo apt-get install -y kubelet=1.30.0-00 kubectl=1.30.0-00
sudo systemctl restart kubelet
kubectl uncordon <worker-node>
```

### **5.6 Step 6: Validate Cluster**

```bash
kubectl get nodes
kubectl get pods -A
kubectl version
```

---

## **6. Testing Backward Compatibility**

* Deploy workloads **before upgrade**:

```bash
kubectl create deployment nginx --image=nginx:1.25
kubectl expose deployment nginx --port=80
```

* Test functionality:

```bash
kubectl exec -it <nginx-pod> -- curl localhost
```

* Verify APIs used by workloads are **not deprecated**.
* Test **ingress, services, configmaps, secrets** to ensure they still work.

---

## **7. Patching Minor Versions**

* Patches fix security issues or minor bugs.
* **Managed clusters**: Provider applies automatically or via console.
* **Self-hosted**: Use rolling patching:

  1. Drain one node
  2. Apply patch (kubeadm, kubelet, kubectl)
  3. Restart kubelet
  4. Uncordon node

**Example Patch Command:**

```bash
sudo apt-get install -y kubelet=1.30.2-00 kubeadm=1.30.2-00 kubectl=1.30.2-00
sudo systemctl restart kubelet
kubectl uncordon <node>
```

---

## **8. Best Practices for Upgrades & Patching**

1. **Backup etcd** before any upgrade.
2. Upgrade **control plane first**, then worker nodes.
3. Use **rolling upgrades** to maintain high availability.
4. Check **deprecated APIs** in manifests.
5. Test workloads **before and after upgrade**.
6. Use **maintenance windows** for production clusters.
7. Monitor cluster post-upgrade using **kubectl, Prometheus, Grafana**.
8. Patch nodes regularly to avoid vulnerabilities.

---

## **9. Troubleshooting Common Issues**

| Problem                     | Solution                                                 |
| --------------------------- | -------------------------------------------------------- |
| Pods crash after upgrade    | Check API deprecations, adjust manifests                 |
| Node stuck in NotReady      | Check kubelet logs, restart kubelet                      |
| Control plane fails upgrade | Ensure etcd snapshot exists, check version compatibility |
| kubeadm upgrade fails       | Verify all nodes run supported versions                  |

---

## **10. Lab Summary**

**Learning Outcomes:**

* Understand **why upgrades and patches are crucial**.
* Learn **upgrade strategies**: rolling, blue-green, canary.
* Perform **managed cluster upgrades** (EKS/GKE/AKS).
* Perform **self-hosted upgrades with kubeadm** (control plane + nodes).
* Test **backward compatibility** and workload functionality.
* Apply **best practices for patching and upgrades**.
* Troubleshoot common upgrade issues.

✅ By the end of this lab, you will confidently **upgrade Kubernetes clusters**, handle patches, and maintain cluster stability for production workloads.


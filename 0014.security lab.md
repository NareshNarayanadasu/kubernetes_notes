Great 👍 Let’s go full deep-dive into **Day 21: Security Lab**.

Here the goal is to **put together everything you learned about security so far**:

* Secure networking (NetworkPolicies)
* Secure access control (RBAC audit)
* Secure workloads (restricted app deployment)

I’ll break it into **concepts → YAML examples → hands-on → best practices → troubleshooting**.

---

# **Day 21: Security Lab**

---

## **1. Apply Network Isolation via Network Policies**

### **1.1 Why Network Policies?**

* By default, **all pods can talk to all other pods** (east-west traffic unrestricted).
* This is a **big risk**: a compromised pod can laterally move.
* **NetworkPolicies** allow you to **whitelist** what pods/services can talk.

---

### **1.2 Key NetworkPolicy Concepts**

* **`podSelector`** → Which pods the policy applies to.
* **`ingress`** → Control inbound traffic (who can talk *to* these pods).
* **`egress`** → Control outbound traffic (where these pods can talk *to*).
* **`namespaceSelector`** → Restrict access between namespaces.

👉 **Note:** Only works if CNI plugin supports it (Calico, Cilium, Weave Net). Not supported in Flannel.

---

### **1.3 Example 1: Deny all traffic**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: dev
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

👉 No pod in `dev` namespace can talk to each other unless explicitly allowed.

---

### **1.4 Example 2: Allow traffic from a frontend to backend**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
```

👉 Only pods labeled `app=frontend` can access pods labeled `app=backend`.

---

### **1.5 Hands-On Commands**

1. Apply deny-all policy:

   ```bash
   kubectl apply -f deny-all.yaml
   ```
2. Test pod-to-pod communication:

   ```bash
   kubectl exec -it <frontend-pod> -- curl http://backend:8080
   ```

   👉 Should fail.
3. Apply frontend-to-backend policy, test again → Should succeed.

---

## **2. Audit RBAC Permissions**

### **2.1 Why Audit RBAC?**

* Over-permissioned accounts = security risk.
* Example: A developer account with `cluster-admin` can delete everything.

---

### **2.2 Check Who Can Do What**

```bash
kubectl auth can-i list pods --as developer -n dev
kubectl auth can-i delete nodes --as developer
```

👉 Returns `yes` or `no`.

---

### **2.3 List All RoleBindings**

```bash
kubectl get rolebindings,clusterrolebindings -A
```

---

### **2.4 Example: Restrict Developer to Pods Only**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: dev-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-rolebinding
  namespace: dev
subjects:
- kind: User
  name: developer
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-role
  apiGroup: rbac.authorization.k8s.io
```

👉 This user cannot touch nodes, services, or secrets.

---

### **2.5 Audit Tools**

* **kubectl-who-can** (plugin) → Shows who can perform an action:

  ```bash
  kubectl-who-can delete pods
  ```
* **rakkess** → Access review CLI (summarizes access).

---

## **3. Deploy a Sample App with Restricted Access**

We combine **network + RBAC + pod security** here.

### **3.1 Restricted App Pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: restricted-app
  namespace: dev
  labels:
    app: restricted
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - name: restricted
    image: nginx:alpine
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop: ["ALL"]
```

👉 Runs non-root, no extra capabilities, read-only filesystem.

---

### **3.2 Restricted Service (Only frontend can access)**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: restricted-svc
  namespace: dev
spec:
  selector:
    app: restricted
  ports:
  - port: 80
    targetPort: 80
```

---

### **3.3 NetworkPolicy to Restrict Access**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restricted-access
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: restricted
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
```

👉 Only pods labeled `frontend` can access this service.

---

## **4. Hands-On Lab Flow**

1. Create `dev` namespace.
2. Apply deny-all policy.
3. Deploy frontend & backend pods → test restricted communication.
4. Apply frontend-to-backend allow policy.
5. Create a `developer` Role + RoleBinding → test with `kubectl auth can-i`.
6. Deploy `restricted-app` with restricted PodSecurity + NetworkPolicy.
7. Validate:

   ```bash
   kubectl exec -it frontend-pod -- curl http://restricted-svc
   ```

   👉 Should work.

   ```bash
   kubectl exec -it backend-pod -- curl http://restricted-svc
   ```

   👉 Should fail.

---

## **5. Best Practices**

✅ Always start with **deny-all policy**, then explicitly allow needed traffic.
✅ Regularly audit **RBAC roles and bindings**.
✅ Assign users **least privilege principle**.
✅ Use **restricted PodSecurity** for apps.
✅ Automate with **OPA/Kyverno** for compliance checks.
✅ Monitor with **Falco** for runtime security alerts.

---

## **6. Troubleshooting**

* Pod cannot reach service:

  * Check if `NetworkPolicy` blocked it.
  * Ensure CNI plugin supports policies.
* User denied permission:

  * Verify Role/ClusterRole vs RoleBinding.
  * Test with `kubectl auth can-i`.
* Restricted pod fails to start:

  * Maybe app needs write access → mount `emptyDir` for writable dirs.

---

## ✅ Key Takeaways

* **NetworkPolicies** = microsegmentation inside cluster.
* **RBAC Audits** = prevent over-privileged users.
* **Restricted Pods** = enforce least privilege workload security.

---

👉 This lab ties together **network, RBAC, and pod-level security** for a **real-world zero-trust Kubernetes setup**.

Would you like me to continue with **Day 22: Monitoring & Logging (Prometheus, Grafana, EFK stack)** in the same in-depth way?

Perfect timing üöÄ Now we‚Äôre on **Day 41‚Äì42: Alerts** ‚Äî where we move from *observing* issues (metrics, logs, traces) to **automatically detecting and notifying** when something goes wrong.

Let‚Äôs go step by step: **concepts ‚Üí Alertmanager setup ‚Üí rules ‚Üí integrations (Slack/Email) ‚Üí hands-on lab ‚Üí best practices ‚Üí troubleshooting**.

---

# **Day 41‚Äì42: Alerts with Alertmanager**

---

## **1. Why Alerts?**

* Metrics/Logs/Traces = passive visibility.
* Alerts = **proactive detection + action**.
* Examples:

  * CPU > 90% for 5 minutes ‚Üí scale up.
  * Node not ready for > 2 minutes ‚Üí investigate.
  * Pod CrashLoopBackOff ‚Üí notify devs.
  * Memory leak detected ‚Üí restart app.

üëâ Without alerts, problems hide until *users* complain.

---

## **2. Components**

* **Prometheus** ‚Üí evaluates alerting rules.
* **Alertmanager** ‚Üí receives alerts and routes them.
* **Receivers** ‚Üí Slack, Email, PagerDuty, Webhooks, etc.

Flow:
**Prometheus ‚Üí (Alert rules fire) ‚Üí Alertmanager ‚Üí Notification channel**

---

## **3. Install Alertmanager**

If you installed **kube-prometheus-stack via Helm** earlier (Day 36‚Äì37), Alertmanager is already included.

Verify:

```bash
kubectl get pods -n monitoring | grep alertmanager
```

Access UI:

```bash
kubectl port-forward svc/kube-monitor-alertmanager 9093:9093 -n monitoring
```

üëâ Open [http://localhost:9093](http://localhost:9093).

---

## **4. Create Alerting Rules**

Example: **CPU usage > 80%**

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cpu-alerts
  namespace: monitoring
spec:
  groups:
  - name: cpu.rules
    rules:
    - alert: HighCPUUsage
      expr: (100 * (1 - avg by(instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])))) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected on {{ $labels.instance }}"
        description: "CPU usage > 80% for more than 2 minutes."
```

Apply:

```bash
kubectl apply -f cpu-alert.yaml
```

---

## **5. Configure Alertmanager Receivers**

### Example: Slack

1. Create a Slack webhook:

   * Go to **Slack ‚Üí Apps ‚Üí Incoming Webhooks ‚Üí Add New**.
   * Copy the webhook URL.

2. Configure Alertmanager (`alertmanager.yaml`):

```yaml
global:
  resolve_timeout: 5m

route:
  receiver: 'slack-notifications'

receivers:
- name: 'slack-notifications'
  slack_configs:
  - send_resolved: true
    channel: '#alerts'
    api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXX'
```

3. Update config:

```bash
kubectl -n monitoring create secret generic alertmanager-kube-monitor-alertmanager --from-file=alertmanager.yaml --dry-run=client -o yaml | kubectl apply -f -
```

4. Restart Alertmanager pod.

---

### Example: Email

Add SMTP config:

```yaml
receivers:
- name: 'email-notifications'
  email_configs:
  - to: 'devops-team@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'smtp.example.com:587'
    auth_username: 'alertmanager@example.com'
    auth_password: 'password123'
```

---

## **6. Hands-On Lab**

1. Deploy a **test app** that consumes lots of CPU (e.g., busybox stress tool).
2. Apply the **HighCPUUsage alert rule**.
3. Watch alert fire in Alertmanager UI.
4. Verify Slack/Email notifications.
5. Resolve app ‚Üí alert auto-closes.

---

## **7. Best Practices**

‚úÖ Always use **for: <time>** in alerts (avoid flapping).
‚úÖ Classify alerts by **severity** (warning/critical).
‚úÖ Use **labels (team, environment, service)** for routing.
‚úÖ Integrate with **Slack/Jira/PagerDuty** depending on escalation.
‚úÖ Avoid **too many alerts** ‚Üí focus on actionable ones.
‚úÖ Group alerts in Alertmanager (e.g., ‚Äú5 pods down in namespace X‚Äù).
‚úÖ Add **runbook links** in `annotations` for faster troubleshooting.

---

## **8. Troubleshooting**

* **No alerts firing?** ‚Üí Check PromQL expression in Prometheus UI.
* **Alerts not reaching Slack/Email?** ‚Üí Check Alertmanager logs.
* **Too many false alerts?** ‚Üí Increase `for:` duration.
* **Alertmanager pod crashloop** ‚Üí bad config in secret.

---



Awesome ðŸš€ Now letâ€™s move into **Day 38â€“39: Logging** â€” where we focus on **centralized log management in Kubernetes**.
Metrics (Prometheus) tell you *what* is happening, but **logs tell you *why***. Without centralized logging, debugging distributed apps across many nodes/pods is nearly impossible.

Iâ€™ll cover **concepts â†’ EFK (Elasticsearch, Fluentd, Kibana) & Loki stack â†’ installation â†’ hands-on labs â†’ log queries â†’ best practices â†’ troubleshooting**.

---

# **Day 38â€“39: Logging in Kubernetes**

---

## **1. Why Centralized Logging Matters**

* Logs are **ephemeral** in Kubernetes:

  * Pods restart â†’ old logs are lost.
  * Pods move between nodes â†’ logs scattered.
* Need a **single place** to collect, store, search, and analyze logs.
* Helps with:

  * Debugging crashes (CrashLoopBackOff).
  * Security/audit trails.
  * Compliance (PCI, HIPAA).
  * Performance troubleshooting.

ðŸ‘‰ Two main approaches:

1. **EFK stack** (Elasticsearch + Fluentd + Kibana).
2. **Loki stack** (Grafana Loki + Promtail + Grafana).

---

## **2. Logging Stacks**

### **EFK Stack**

* **Elasticsearch** â†’ stores logs (searchable DB).
* **Fluentd** â†’ collects & ships logs from nodes/pods.
* **Kibana** â†’ UI for searching and visualizing logs.

### **Loki Stack**

* **Loki** â†’ lightweight log aggregation system.
* **Promtail** â†’ collects logs and ships to Loki.
* **Grafana** â†’ visualize logs alongside metrics.

ðŸ‘‰ Loki is **cheaper and more lightweight** (stores indexes efficiently).
ðŸ‘‰ EFK is **richer but heavier** (requires large storage & resources).

---

## **3. Kubernetes Logging Basics**

* Logs are written to **stdout/stderr** inside containers.
* Kubelet writes logs to files on nodes:

  * `/var/log/pods/<namespace>_<pod>/<container>.log`
* `kubectl logs` fetches from these files via API server.

---

## **4. Install Loki (lightweight option)**

### Step 1: Install via Helm

```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Install Loki + Promtail + Grafana
helm install loki grafana/loki-stack --namespace logging --create-namespace
```

### Step 2: Verify Pods

```bash
kubectl get pods -n logging
```

### Step 3: Access Grafana

```bash
kubectl port-forward svc/loki-grafana 3000:80 -n logging
```

ðŸ‘‰ Login: `admin/admin`.
ðŸ‘‰ Add **Loki** as data source.

### Step 4: Query Logs

In Grafana â†’ Explore â†’ Loki â†’ run queries:

```logql
{app="nginx"}
```

---

## **5. Install EFK (heavier but powerful)**

### Step 1: Deploy Elasticsearch

```bash
kubectl apply -f https://download.elastic.co/downloads/eck/2.4.0/all-in-one.yaml
```

### Step 2: Deploy Fluentd DaemonSet

* Collects logs from `/var/log/containers/` on each node.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
```

### Step 3: Deploy Kibana

```bash
kubectl apply -f https://raw.githubusercontent.com/elastic/helm-charts/main/kibana/values.yaml
```

### Step 4: Access Kibana

```bash
kubectl port-forward svc/kibana 5601:5601 -n logging
```

ðŸ‘‰ Go to **Discover** tab â†’ search logs with filters.

---

## **6. Multi-Container Pod Logging**

For pods with multiple containers:

```bash
# View logs for one container
kubectl logs <pod-name> -c <container-name>

# Tail logs for all containers
kubectl logs -f <pod-name> --all-containers=true
```

ðŸ‘‰ In Fluentd/Promtail setup, logs from **all containers** are collected automatically.

---

## **7. Hands-On Lab**

1. Deploy a **multi-container pod** (nginx + sidecar).
2. Check logs with `kubectl logs`.
3. Send logs to **Loki** via Promtail.
4. Query logs in Grafana.
5. Deploy a **crashing pod** and see logs in Kibana/Loki.

---

## **8. Best Practices**

âœ… Always log to **stdout/stderr** (donâ€™t write logs inside container filesystem).
âœ… Use **structured logging (JSON)** for easier parsing.
âœ… Apply **log retention policies** (Loki/Elasticsearch).
âœ… Use **labels** (app, namespace, pod) for filtering logs.
âœ… Monitor **log pipeline health** (Fluentd crashes â†’ lost logs).
âœ… Use Loki for **cost-effective logging**, EFK for **enterprise-grade search**.

---

## **9. Troubleshooting**

* **Logs missing?** â†’ Check Fluentd/Promtail DaemonSet.
* **Elasticsearch high memory usage?** â†’ tune JVM, set index retention.
* **Grafana no logs?** â†’ verify data source config.
* **Pod logs empty?** â†’ container not writing to stdout/stderr.

---

##

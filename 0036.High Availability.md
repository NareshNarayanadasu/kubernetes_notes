Here’s a **detailed, text-only guide for Day 64–65: High Availability**, covering all concepts and hands-on steps for in-depth understanding.

---

# **Day 64–65: High Availability (HA) in Kubernetes**

---

## **1. Lab Objective**

* Understand **High Availability (HA)** concepts in Kubernetes.
* Deploy **multi-replica control plane** (conceptually in lab).
* Configure **HA applications** using **StatefulSets and Deployments**.
* Simulate failures and verify **resiliency**.

---

## **2. Why High Availability?**

* Prevent **single points of failure** in both control plane and applications.
* Ensure **application uptime** during maintenance or node failure.
* Critical for **production workloads** requiring 24/7 availability.

---

## **3. HA Kubernetes Control Plane**

* Control plane components: **API Server, etcd, Controller Manager, Scheduler**.
* HA setup usually requires **3+ control plane nodes**.
* In labs, we simulate HA using **multi-node clusters** or **kind with multiple control-plane replicas**.

**Key points:**

* **API Server:** load-balanced across replicas.
* **etcd:** cluster must have an odd number of members (3, 5, …) for quorum.
* **Controller & Scheduler:** typically **leader-election enabled**; multiple replicas for HA.

**Lab Simulation (Kind example):**

```bash
kind create cluster --name ha-cluster --config kind-ha.yaml
```

`kind-ha.yaml` defines **3 control-plane nodes**.

---

## **4. HA Application Principles**

* Use **multiple replicas** for Deployments/StatefulSets.
* Use **PodDisruptionBudgets (PDBs)** to maintain minimum availability during node drains.
* Configure **readiness and liveness probes** for automatic recovery.
* Distribute pods across nodes using **affinity, anti-affinity, taints/tolerations**.

---

## **5. Hands-On: HA Applications with StatefulSets**

### **5.1 Create StatefulSet**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ha-db
  namespace: ha-lab
spec:
  serviceName: "ha-db-svc"
  replicas: 3
  selector:
    matchLabels:
      app: ha-db
  template:
    metadata:
      labels:
        app: ha-db
    spec:
      containers:
      - name: postgres
        image: postgres:14
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          value: "mypassword"
        volumeMounts:
        - name: pgdata
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: pgdata
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
```

```bash
kubectl create ns ha-lab
kubectl apply -f ha-db-statefulset.yaml
kubectl get pods -n ha-lab -o wide
```

---

### **5.2 Create Headless Service for StatefulSet**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ha-db-svc
  namespace: ha-lab
spec:
  clusterIP: None
  selector:
    app: ha-db
  ports:
  - port: 5432
    targetPort: 5432
```

```bash
kubectl apply -f ha-db-svc.yaml
```

* Headless service allows **stable network identities** for each pod (`ha-db-0`, `ha-db-1`, …).

---

## **6. PodDisruptionBudget (PDB) for HA**

* Ensures **minimum number of pods remain available** during maintenance.

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ha-db-pdb
  namespace: ha-lab
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: ha-db
```

```bash
kubectl apply -f ha-db-pdb.yaml
kubectl get pdb -n ha-lab
```

* During a **node drain**, Kubernetes will respect the PDB and not evict pods below `minAvailable`.

---

## **7. Simulate Failure**

1. Cordon & drain a node:

```bash
kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```

2. Observe StatefulSet pods rescheduling on other nodes:

```bash
kubectl get pods -n ha-lab -o wide
```

* PDB ensures **at least 2 pods remain available**.

---

## **8. High Availability Considerations**

| Component     | HA Strategy                                                 |
| ------------- | ----------------------------------------------------------- |
| Control Plane | 3+ API servers behind load balancer, etcd quorum            |
| StatefulSet   | Multiple replicas, PDBs, stable network identities          |
| Deployments   | Multiple replicas, readiness/liveness probes, anti-affinity |
| Services      | LoadBalancer or ClusterIP with DNS for redundancy           |
| Storage       | Use persistent volumes with HA storage (NFS, cloud storage) |

---

## **9. Best Practices**

1. Always have **odd-numbered etcd members** for quorum.
2. Use **PodDisruptionBudgets** for critical applications.
3. Spread replicas across **nodes/zones/regions** for resilience.
4. Monitor HA applications with **Prometheus/Grafana**.
5. Test failure scenarios regularly (node down, pod eviction).

---

## **10. Lab Summary**

* Simulated **HA control plane** and deployed HA applications.
* Configured **StatefulSets with headless services** for stable identities.
* Added **PodDisruptionBudgets** to maintain minimum availability.
* Simulated **node failures** to test resiliency.
* Learned **key HA principles for Kubernetes production workloads**.

✅ **Outcome:**

* Understand **HA design for clusters and applications**.
* Ability to **deploy resilient workloads** with StatefulSets, PDBs, and multi-replica deployments.
* Ready to implement **production-grade Kubernetes HA environments**.

---


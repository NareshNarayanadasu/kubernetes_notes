
# **Day 1–2: Kubernetes Cluster Architecture Deep Dive (In-Depth)**

Kubernetes is a **container orchestration platform** that manages the lifecycle of containers across a cluster of nodes. A Kubernetes cluster has **two main components**:

1. **Control Plane (Master components)**
2. **Worker Nodes (Node components)**

---

## **1. Control Plane (Master Node)**

The **control plane** is responsible for **managing the cluster state, scheduling pods, and responding to events**.

### **1.1 kube-apiserver**

* **Role:** The central entry point to the cluster (REST API).
* **Responsibilities:**

  * Exposes Kubernetes API.
  * Validates and configures data for API objects (pods, services, deployments).
  * Communicates with **etcd** to persist cluster state.
* **Key Notes:**

  * Every component communicates with `kube-apiserver`.
  * Exposes endpoints like `https://<master-ip>:6443`.

**Hands-On Check:**

```bash
kubectl cluster-info
kubectl get componentstatuses
kubectl get nodes -o wide
```

---

### **1.2 etcd**

* **Role:** Distributed key-value store for Kubernetes.
* **Responsibilities:**

  * Stores **all cluster state** (pods, config, secrets, nodes, deployments).
  * Provides strong consistency (Raft consensus protocol).
* **Key Notes:**

  * Losing etcd data can break the cluster.
  * Backup & restore is critical.

**Hands-On Check:**

```bash
ETCDCTL_API=3 etcdctl get / --prefix --keys-only
```

---

### **1.3 kube-scheduler**

* **Role:** Assigns pods to nodes.
* **Responsibilities:**

  * Looks at pods without nodes assigned.
  * Chooses the **best-fit node** based on resources, affinity, taints/tolerations.
* **Scheduling Considerations:**

  * CPU/Memory requests
  * Node affinity/anti-affinity
  * Taints & tolerations
  * Pod topology spread constraints

---

### **1.4 kube-controller-manager**

* **Role:** Runs controller processes that **regulate cluster state**.
* **Controllers include:**

  * **Node controller:** Monitors node health.
  * **Replication controller:** Ensures correct number of pod replicas.
  * **Deployment controller:** Handles rolling updates.
  * **Job controller:** Manages batch jobs.
* **Key Notes:** Runs all controllers as a single process.

---

### **1.5 cloud-controller-manager** (if cloud provider used)

* **Role:** Integrates K8s with cloud provider APIs (AWS, GCP, Azure).
* **Responsibilities:**

  * Manages nodes, load balancers, storage volumes in cloud.
* **Separation:** Allows K8s core to be cloud-agnostic.

---

## **2. Worker Nodes (Node Components)**

Worker nodes run the actual workloads (pods). Each node has several components:

### **2.1 kubelet**

* **Role:** Agent that runs on each node.
* **Responsibilities:**

  * Ensures containers in pods are running and healthy.
  * Reports status to the kube-apiserver.
  * Watches for pod definitions assigned to this node.
* **Key Notes:**

  * Cannot schedule pods; only runs them.

**Hands-On Check:**

```bash
kubectl describe node <node-name>
```

---

### **2.2 kube-proxy**

* **Role:** Handles networking rules for pods and services.
* **Responsibilities:**

  * Maintains network rules (iptables/ipvs) on nodes.
  * Enables communication between pods, services, and external clients.
* **Key Notes:**

  * Supports ClusterIP, NodePort, LoadBalancer service types.
  * Manages service discovery internally.

---

### **2.3 Container Runtime**

* **Role:** Runs containers (Docker, containerd, CRI-O).
* **Responsibilities:**

  * Pulls container images.
  * Runs & stops containers.
  * Reports container status to kubelet.
* **Key Notes:**

  * Must conform to **CRI (Container Runtime Interface)**.

---

## **3. Cluster Data Flow**

1. User interacts with **kubectl → kube-apiserver**.
2. API server validates request → stores in **etcd**.
3. **Scheduler** assigns pods to nodes based on resource & policy.
4. **Controller Manager** ensures desired state matches actual state.
5. **kubelet** on nodes pulls container images via container runtime.
6. **kube-proxy** sets networking rules so pods can communicate.
7. **Monitoring & logs** flow through metrics/logging stack (Prometheus, EFK).

**Diagram (Conceptual Flow):**

```
User (kubectl)
     |
     v
kube-apiserver ----> etcd
     |
     v
kube-scheduler & kube-controller-manager
     |
     v
Worker Node: kubelet -> container runtime -> Pods
     |
     v
kube-proxy -> Pod networking
```

---

## **4. Cluster High Availability**

* **Control Plane HA:**

  * Multiple API servers with a load balancer.
  * etcd in cluster mode (3,5,7 nodes).
* **Node HA:**

  * Multiple worker nodes for redundancy.
  * Deployments/ReplicaSets ensure multiple pod replicas.

---

## **5. Hands-On Exercises**

1. **Inspect Cluster Components**

```bash
kubectl get pods -n kube-system
kubectl get nodes -o wide
kubectl describe node <node-name>
```

2. **Check API Server Connectivity**

```bash
kubectl cluster-info
curl -k https://<master-ip>:6443/version
```

3. **Explore etcd**

```bash
ETCDCTL_API=3 etcdctl get / --prefix --keys-only
```

4. **Simulate Scheduler Behavior**

* Create a pod with resource requests higher than any node, see it stay pending:

```yaml
resources:
  requests:
    cpu: "4"
    memory: "8Gi"
```

```bash
kubectl apply -f pod.yaml
kubectl describe pod <pod-name>
```

---

## **6. Key Concepts to Remember**

* **Control Plane:** Brain of the cluster, manages desired state.
* **Worker Nodes:** Run your actual applications.
* **etcd:** Source of truth for cluster state, must be backed up.
* **kube-apiserver:** All communication goes through it.
* **Scheduler & Controller Manager:** Ensure workloads are correctly placed and maintained.
* **kubelet & kube-proxy:** Ensure pods are running and networked correctly.
* **Container Runtime:** Runs containers (Docker/containerd).

---



---



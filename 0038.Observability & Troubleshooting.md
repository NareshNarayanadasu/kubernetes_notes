Here’s a **detailed, text-only guide for Day 68–69: Observability & Troubleshooting**, fully covering concepts, tools, and hands-on steps for in-depth understanding.

---

# **Day 68–69: Observability & Troubleshooting in Kubernetes**

---

## **1. Lab Objective**

* Learn how to **monitor Kubernetes clusters** and applications.
* Understand **metrics, logging, and tracing** for troubleshooting.
* Simulate failures (pod crash, node failure, network issues) and **diagnose problems**.
* Use **kubectl, Prometheus, Grafana, and logging tools** for observability.

---

## **2. Key Concepts**

| Concept       | Description                                                                    |
| ------------- | ------------------------------------------------------------------------------ |
| Observability | Ability to understand cluster behavior via metrics, logs, and traces           |
| Metrics       | Quantitative data (CPU, memory, request latency)                               |
| Logging       | Capturing events and application output for debugging                          |
| Tracing       | Following request flow through multiple services (e.g., Jaeger, OpenTelemetry) |
| Events        | Kubernetes internal events for resource changes and failures                   |

---

## **3. Tools for Observability**

### **3.1 Prometheus & Grafana (Metrics)**

* **Prometheus** scrapes metrics from nodes, kubelets, and pods.
* **Grafana** visualizes metrics with dashboards.

**Installation Example (Helm):**

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack
```

* Access Grafana dashboard via port-forwarding:

```bash
kubectl port-forward svc/prometheus-grafana 3000:80
```

* Useful metrics: CPU usage, memory usage, pod restarts, network latency.

---

### **3.2 Logging (EFK / Loki)**

* Centralized logging helps **trace errors across multiple pods**.
* **EFK stack:** Elasticsearch (store) + Fluentd (collector) + Kibana (visualize)
* **Loki:** Lightweight alternative for logs with Grafana integration.

**Example: Loki Deployment**

```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm install loki grafana/loki-stack
kubectl logs <fluentd-pod> -n loki
```

---

### **3.3 Tracing (Jaeger / OpenTelemetry)**

* Trace requests across **microservices** to locate bottlenecks.
* Useful for **latency troubleshooting**.

**Jaeger example:**

```bash
kubectl apply -f https://www.jaegertracing.io/latest/operator/deploy/examples/jaeger-all-in-one.yaml
kubectl port-forward svc/jaeger 16686:16686
```

* Visualize request flow and detect failed services.

---

## **4. Hands-On: Simulate Failures**

### **4.1 Pod Crash**

1. Deploy test pod:

```bash
kubectl run crash-test --image=busybox --restart=Never -- /bin/sh -c "exit 1"
kubectl get pods
kubectl describe pod crash-test
kubectl logs crash-test
```

* Observe pod crash and restarts (if part of Deployment).

2. Metrics & alerts:

* Prometheus can detect **high pod restart count**.

---

### **4.2 Node Failure**

1. Simulate node cordon/drain:

```bash
kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```

2. Observe pod rescheduling:

```bash
kubectl get pods -o wide
```

3. Metrics:

* Node CPU/Memory unavailable alerts in Grafana.

---

### **4.3 Network Failure**

* Simulate network partition using **network policies** or **iptables**:

```bash
kubectl exec -it <pod> -- /bin/sh
ping <other-pod-ip>  # Should fail if network blocked
```

* Use metrics and logs to verify:

  * **Failed connections**
  * **Increased request latency**

---

## **5. Troubleshooting Steps**

### **5.1 Using kubectl**

* Check pod status:

```bash
kubectl get pods -n <namespace>
kubectl describe pod <pod-name>
kubectl logs <pod-name>
```

* Check events:

```bash
kubectl get events -n <namespace> --sort-by='.metadata.creationTimestamp'
```

* Check node health:

```bash
kubectl get nodes
kubectl describe node <node-name>
```

---

### **5.2 Using Metrics & Alerts**

* View CPU/Memory usage in **Grafana**.
* Set **alerts** for high usage or pod restarts via **Alertmanager**.

---

### **5.3 Using Logs**

* Aggregated logs help identify:

  * Crash reasons
  * Misconfigurations
  * Failed API calls

```bash
kubectl logs <pod-name> -c <container-name>
```

* Search logs in **Elasticsearch/Kibana** or **Loki/Grafana**.

---

### **5.4 Using Tracing**

* Follow request across services to detect **slow or failed service calls**.
* Identify bottlenecks or microservice failures.

---

## **6. Best Practices**

1. Always deploy **centralized logging and metrics**.
2. Set up **alerts** for critical thresholds (CPU, memory, restarts).
3. Regularly **simulate failures** to test cluster resilience.
4. Use **readiness/liveness probes** for self-healing pods.
5. Monitor **network, storage, and control plane** health.
6. Keep dashboards and alerts **up-to-date with workload changes**.

---

## **7. Troubleshooting Workflow Example**

1. Pod in CrashLoopBackOff:

   * `kubectl describe pod` → check events
   * `kubectl logs` → check error message
   * Check container image or command

2. Node unready:

   * `kubectl describe node` → check taints, conditions
   * Check kubelet logs on node
   * Reschedule pods to healthy nodes

3. Service unreachable:

   * `kubectl exec` ping test
   * Check **NetworkPolicies** or **Service endpoints**
   * Verify pod labels match service selector

---

## **8. Lab Summary**

* Learned to **simulate failures**: pod crash, node failure, network failure.
* Used **metrics, logs, and tracing** to troubleshoot issues.
* Applied **kubectl, Prometheus, Grafana, Loki, and Jaeger** for observability.
* Developed a **structured troubleshooting workflow** for production clusters.

✅ **Outcome:**

* Can **detect, diagnose, and resolve cluster and application issues**.
* Understand how to **combine metrics, logs, and traces** for root cause analysis.
* Ready to implement **production-grade observability and troubleshooting strategies** in Kubernetes.



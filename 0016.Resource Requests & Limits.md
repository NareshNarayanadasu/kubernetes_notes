Great ðŸš€ Now letâ€™s go deep into **Week 5 â€“ Resource Management & Scheduling** starting with
**Day 29â€“30: Resource Requests & Limits**.

This is one of the **most important Kubernetes fundamentals** â€” because misconfigured CPU/memory leads to performance issues, pod evictions, or even cluster outages.

Iâ€™ll break it down as **concepts â†’ YAML â†’ hands-on lab â†’ monitoring â†’ best practices â†’ troubleshooting**.

---

# **Day 29â€“30: Resource Requests & Limits**

---

## **1. Why Requests & Limits Matter**

* Kubernetes schedules pods **based on resource requests**.
* **Requests** = Minimum guaranteed resources (scheduler uses this).
* **Limits** = Maximum a container can use (enforced at runtime by kubelet/cgroups).

ðŸ‘‰ Without them:

* Pods may overuse node resources.
* No fair resource allocation â†’ noisy-neighbor problem.
* Mission-critical apps can starve.

---

## **2. Key Concepts**

| Concept             | Meaning                 | Example                                |
| ------------------- | ----------------------- | -------------------------------------- |
| **CPU unit**        | 1 CPU = 1 vCPU/Core     | `1000m = 1 CPU`, `500m = 0.5 CPU`      |
| **Memory unit**     | Bytes (`Mi`, `Gi`)      | `128Mi = 128 Mebibytes`                |
| **requests.cpu**    | Minimum CPU reserved    | Scheduler ensures node has â‰¥ that much |
| **limits.cpu**      | Max CPU allowed         | Container throttled if it exceeds      |
| **requests.memory** | Minimum memory reserved | Scheduler ensures node has â‰¥ that much |
| **limits.memory**   | Max memory allowed      | Container killed if it exceeds         |

---

## **3. Example Pod with Requests & Limits**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "while true; do echo running; sleep 5; done"]
    resources:
      requests:
        cpu: "100m"       # 0.1 CPU guaranteed
        memory: "128Mi"   # 128MB guaranteed
      limits:
        cpu: "500m"       # max 0.5 CPU
        memory: "512Mi"   # max 512MB
```

ðŸ‘‰ Scheduler places pod only if a node has **â‰¥100m CPU + 128Mi memory free**.
ðŸ‘‰ At runtime, pod can burst up to 500m CPU, but if it tries to exceed 512Mi memory â†’ killed (OOMKilled).

---

## **4. Hands-On Lab**

### Step 1: Deploy Pod with Requests & Limits

```bash
kubectl apply -f resource-demo.yaml
```

### Step 2: Describe Pod

```bash
kubectl describe pod resource-demo
```

ðŸ‘‰ See requests & limits under **Containers â†’ Resources**.

### Step 3: Generate CPU Load

```bash
kubectl exec -it resource-demo -- sh -c "yes > /dev/null"
```

ðŸ‘‰ CPU will max out â†’ throttled at 500m.

### Step 4: Generate Memory Load

```bash
kubectl exec -it resource-demo -- sh -c "dd if=/dev/zero of=/dev/shm/test bs=1M count=1024"
```

ðŸ‘‰ If >512Mi memory used â†’ pod gets **OOMKilled**.

---

## **5. Resource Management Strategies**

### **5.1 Requests < Limits**

* Allows **bursting** (good for apps with variable load).
* Scheduler ensures minimum, runtime allows more.

### **5.2 Requests = Limits**

* Guarantees app **never gets throttled or OOMKilled unexpectedly**.
* Safer for critical apps (databases).

### **5.3 No Requests/Limits**

* Pod can use any available node resources.
* Risk: **resource starvation** for other pods.

---

## **6. Monitoring Resource Usage**

* Check pod usage:

```bash
kubectl top pod
kubectl top node
```

* Visualize in Prometheus/Grafana â†’ track CPU throttling, memory usage vs. limits.

---

## **7. Best Practices**

âœ… Always define **requests & limits** for all pods.
âœ… For critical workloads â†’ set `requests = limits`.
âœ… For burstable workloads â†’ set lower requests, higher limits.
âœ… Use **ResourceQuotas** (namespace-level) to prevent abuse.
âœ… Use **LimitRanges** (default per-container request/limit).
âœ… Monitor **OOMKilled events** â†’ increase memory limits if frequent.
âœ… Benchmark app under load â†’ set realistic values, not guesses.

---

## **8. Troubleshooting**

* **Pod Pending** â†’ Node doesnâ€™t have enough requested CPU/memory.
* **Pod OOMKilled** â†’ Memory limit too low.
* **Pod throttled** â†’ CPU limit too low.
* **Cluster overprovisioned** â†’ Too high requests block scheduling.
* **Cluster underutilized** â†’ Requests too low, but pods hog runtime resources.

---

## âœ… Key Takeaways

* **Requests = scheduler guarantee**; **Limits = runtime enforcement**.
* Correct sizing ensures **stability, performance, and fair usage**.
* Use monitoring + load testing to tune requests/limits.
* Combine with **ResourceQuotas & LimitRanges** for namespace-wide fairness.

---

